AWSTemplateFormatVersion: '2010-09-09'
Description: 'Device Data Generator'
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: 'Job Configuration'
        Parameters:
          - GenerationState
          - GenerationInterval
          - TotalDevices
          - DevicesPerWorker
          - RegionalVoltage
          - MinLoad
          - MaxLoad
      - Label:
          default: 'Timestream Configuration'
        Parameters:
          - TimestreamCreation
          - TimestreamDatabaseName
          - TimestreamTableName
    ParameterLabels: 
      GenerationState:
        default: 'What is the desired state generating device readings?'
      GenerationInterval:
        default: 'How frequently do you want to generate device readings?'
      TotalDevices:
        default: 'For how many devices do you want to generate readings at each interval?'
      DevicesPerWorker:
        default: 'How many devices should each Lambda worker handle?'
      RegionalVoltage:
        default: 'What is the voltage for which to base the device readings?'
      MinLoad:
        default: 'What is the minimum contract load KW for a device?'
      MaxLoad:
        default: 'What is the maximum contract load KW for a device?'
      TimestreamCreation:
        default: 'Do you want to use an existing Timestream table or create a new one?'
      TimestreamDatabaseName:
        default: 'Name of the new or existing Timestream database'
      TimestreamTableName:
        default: 'Name of the new or existing Timestream table'
Parameters:
  GenerationState:
    Description: 'Change to DISABLE at any time to stop generation of device readings'
    Type: String
    Default: 'ENABLED'
    AllowedValues:
      - 'ENABLED'
      - 'DISABLED'
  GenerationInterval:
    Description: 'Minutes per interval, must be greater than 5'
    Type: Number
    Default: 5
    MinValue: 5
    ConstraintDescription: 'GenerationInterval must contain a numeric value greater than 5'
  TotalDevices:
    Description: 'Number of devices to generate readings at each interval'
    Type: Number
    Default: 50000
  DevicesPerWorker:
    Description: 'A higher number means less Lambda workers invoked, but longer time to generate readings'
    Type: Number
    Default: 10000
  RegionalVoltage:
    Description: 'Number of volts for country'
    Type: Number
    Default: 220
  MinLoad:
    Description: 'Minimum contract load KW'
    Type: Number
    Default: 5
    MinValue: 0
    MaxValue: 100
  MaxLoad:
    Description: 'Maximum contract load KW'
    Type: Number
    Default: 20
    MinValue: 0
    MaxValue: 100
  TimestreamCreation:
    Default: 'Create New'
    Type: String
    AllowedValues:
      - 'Create New'
      - 'Use Existing'
  TimestreamDatabaseName:
    Type: String
    Default: 'devices'
    MinLength: 3
    MaxLength: 256
  TimestreamTableName:
    Type: String
    Default: 'readings'
    MinLength: 3
    MaxLength: 256
Conditions:
  createTimestream: !Equals [ !Ref TimestreamCreation, 'Create New']
Resources:
  TimestreamDatabase:
    Condition: createTimestream
    Type: AWS::Timestream::Database
    Properties:
      DatabaseName: !Ref TimestreamDatabaseName
  TimestreamTable:
    Condition: createTimestream
    Type: AWS::Timestream::Table
    Properties:
      DatabaseName: !Ref TimestreamDatabase
      TableName: !Ref TimestreamTableName
      RetentionProperties:
        MemoryStoreRetentionPeriodInHours: 168
        MagneticStoreRetentionPeriodInDays: 30
  WorkerSQSQueue:
    Type: AWS::SQS::Queue
    Properties:
      VisibilityTimeout: 3600
      MessageRetentionPeriod: 28800
  OrchestratorLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          Effect: 'Allow'
          Action:
            - 'sts:AssumeRole'
          Principal:
            Service:
              - lambda.amazonaws.com
        Version: '2012-10-17'
      Policies:
        - PolicyName: 'root'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'sqs:SendMessage'
                  - 'sqs:GetQueueAttributes'
                Resource: !GetAtt WorkerSQSQueue.Arn
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: '*'
  WorkerLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          Effect: 'Allow'
          Action:
            - 'sts:AssumeRole'
          Principal:
            Service:
              - lambda.amazonaws.com
        Version: '2012-10-17'
      Policies:
        - PolicyName: 'root'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'sqs:ReceiveMessage'
                  - 'sqs:DeleteMessage'
                  - 'sqs:GetQueueAttributes'
                Resource: !GetAtt WorkerSQSQueue.Arn
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: '*'
              - Effect: 'Allow'
                Action:
                  - 'timestream:DescribeEndpoints'
                Resource: '*'
              - Effect: 'Allow'
                Action:
                  - 'timestream:WriteRecords'
                Resource: !If [ createTimestream, !GetAtt TimestreamTable.Arn, !Join [ '/', [ !Join [ ':', ['arn' , 'aws', 'timestream', !Ref AWS::Region, !Ref AWS::AccountId, 'database'] ] , !Ref TimestreamDatabaseName, 'table', !Ref TimestreamTableName ] ] ]
  SQSLambdaTrigger:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: 1
      EventSourceArn: !GetAtt WorkerSQSQueue.Arn
      FunctionName: !GetAtt WorkerLambdaFunction.Arn
  OrechestratorScheduledRule:
    Type: AWS::Events::Rule
    Properties:
      Description: !Join [ ' ', [ 'Launch', !Ref OrchestratorLambdaFunction, 'every', !Ref GenerationInterval, 'minutes' ] ]
      ScheduleExpression: !Join [ '', [ 'rate(', !Ref GenerationInterval, ' minutes)' ] ]
      State: !Ref GenerationState
      Targets:
        - Arn: !GetAtt OrchestratorLambdaFunction.Arn
          Id: 'TargetFunctionV1'
  PermissionForEventsToInvokeOrchestratorLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref OrchestratorLambdaFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt OrechestratorScheduledRule.Arn
  WorkerLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt WorkerLambdaRole.Arn
      Runtime: python3.9
      MemorySize: 128
      Timeout: 900
      Environment:
        Variables:
          TIMESTREAM_DATABASE: !If [createTimestream, !Ref TimestreamDatabase, !Ref TimestreamDatabaseName]
          TIMESTREAM_TABLE: !If [createTimestream, !GetAtt TimestreamTable.Name, !Ref TimestreamTableName]
          REGION_VOLTAGE: !Ref RegionalVoltage
          MIN_LOAD: !Ref MinLoad
          MAX_LOAD: !Ref MaxLoad
      Handler: 'index.lambda_handler'
      Code:
        ZipFile: |
          import os
          import json
          import uuid
          import boto3
          import random
          import botocore
          from datetime import datetime

          timestream_database = os.environ['TIMESTREAM_DATABASE']
          timestream_table = os.environ['TIMESTREAM_TABLE']
          region_voltage = int(os.environ['REGION_VOLTAGE']) #country-specific reference voltage
          min_load = float(os.environ['MIN_LOAD']) #minimum load across all devices
          max_laod = float(os.environ['MAX_LOAD']) #maximum load across all devices

          session = boto3.Session()
          write_client = session.client('timestream-write')
          write_batch_size = 100

          def parse_data_type(data_type,string_value):
            try:
              if data_type == 'Number':
                string_value = int(string_value)
              else:
                string_value = str(string_value)
              return string_value
            except Exception as e:
              raise ValueError(e) from None

          def generate_readings(device_index):
            try:
              device_id = str(uuid.uuid3(uuid.NAMESPACE_OID, str(device_index)))
              readings = {
                  'Dimensions': [{'Name': 'device_id', 'Value': device_id, 'DimensionValueType': 'VARCHAR'}],
                  'MeasureValues': []
                }

              #create persistent load in range for device based on device_index
              random.seed(device_index) #set seed
              load = int(random.uniform(min_load, max_laod))
              random.seed(None) #unset seed

              pf = random.uniform(0.8, 0.99)
              ldf = random.uniform(0, 10)

              kw = load * (ldf / 10)
              kva = kw / pf
              vltg = (random.uniform(0, 1) * (round(region_voltage * 1.1, 2) - round(region_voltage * 0.9, 2)) + round(region_voltage * 0.9, 2)) / 1000
              crrnt = kva / vltg
              
              readings['MeasureValues'].append({'Name': 'load', 'Value': str(load), 'Type': 'BIGINT'})
              readings['MeasureValues'].append({'Name': 'pf', 'Value': str(round(pf, 3)), 'Type': 'DOUBLE'})
              readings['MeasureValues'].append({'Name': 'kw', 'Value': str(round(kw, 3)), 'Type': 'DOUBLE'})
              readings['MeasureValues'].append({'Name': 'kva', 'Value': str(round(kva, 3)), 'Type': 'DOUBLE'})
              readings['MeasureValues'].append({'Name': 'vltg', 'Value': str(round(vltg, 3)), 'Type': 'DOUBLE'})
              readings['MeasureValues'].append({'Name': 'crrnt', 'Value': str(round(crrnt, 3)), 'Type': 'DOUBLE'})
              return readings

            except Exception as e:
              raise ValueError(e) from None

          def write_records(records, common_attributes):
            try:
              result = write_client.write_records(
                DatabaseName = timestream_database,
                TableName = timestream_table,
                Records = records,
                CommonAttributes = common_attributes
              )

            except botocore.exceptions.ClientError as e:
              raise ValueError(e) from None

          def lambda_handler(event, context):
            try:
              # Parse lambda event
              for record in event['Records']:
                item = {}
                for k,v in record['messageAttributes'].items():
                  item[k] = parse_data_type(v['dataType'], v['stringValue'])

                job_start = datetime.now()
                job_log = {
                  'batch_id': item['BatchId'],
                  'batch_time': item['BatchTime'],
                  'start': job_start.replace(microsecond=0).isoformat(),
                  'records_total': 0,
                  'timestream_writes': 0
                }
                
                # Generate records for given range and write in batches to Timestream
                for i in range(item['RangeStart'], item['RangeEnd'] + 1, write_batch_size):
                  common_attributes = {
                    #'Dimensions': [{'Name': 'batch_id', 'Value': item['BatchId']}],
                    'Time': str(item['BatchTime']),
                    'MeasureName': 'readings', 'MeasureValueType': 'MULTI'
                  }
                  records = []
                  for device_index in range(i, min(i + write_batch_size - 1, item['RangeEnd']) + 1):
                    record = generate_readings(device_index)
                    job_log['records_total'] += 1
                    records.append(record)
                  write_records(records, common_attributes)
                  job_log['timestream_writes'] += 1

                # Job Logging
                job_end = datetime.now()
                job_log['end'] = job_end.replace(microsecond=0).isoformat()
                job_duration = job_end - job_start
                job_log['duration_sec'] = (job_duration).total_seconds()
                job_log['records_avg_per_sec'] = float('{:.2f}'.format(job_log['records_total']/job_duration.total_seconds()))
                print(json.dumps(job_log))
            except ValueError as e:
              print(json.dumps(str(e)))
  OrchestratorLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt OrchestratorLambdaRole.Arn
      Runtime: python3.9
      MemorySize: 128
      Timeout: 120
      Environment:
        Variables:
          WORKER_QUEUE_URL: !Ref WorkerSQSQueue
          DEVICE_COUNT: !Ref TotalDevices
          RECORDS_PER_WORKER: !Ref DevicesPerWorker
      Handler: 'index.lambda_handler'
      Code:
        ZipFile: |
          import os
          import json
          import uuid
          import boto3
          from datetime import datetime

          # Job Settings
          queue_url = os.environ['WORKER_QUEUE_URL']
          device_count = int(os.environ['DEVICE_COUNT'])
          records_per_worker = int(os.environ['RECORDS_PER_WORKER'])

          sqs_client = boto3.client('sqs')

          def write_sqs(jobs):
            job_log = {'total_readings': 0, 'sqs_messages': 0, 'sqs_api_calls': 0}
            sqs_batch_size = 10
            chunks = [jobs[x:x+sqs_batch_size] for x in range(0, len(jobs), sqs_batch_size)]
            for chunk in chunks:
              job_log['sqs_api_calls'] += 1
              dt = datetime.now().replace(microsecond=0)
              dt_epoch = datetime.now().replace(microsecond=0).timestamp()
              dt_friendly = datetime.now().replace(microsecond=0).isoformat()
              entries = []
              for x in chunk:
                job_log['sqs_messages'] += 1
                job_log['total_readings'] += x['end'] - x['start']
                entry = {
                  'Id': '-'.join(['WorkerReadings',str(x['start']),str(x['end']),str(int(dt_epoch))]),
                  'MessageBody': 'Worker reading generation request for devices ' + str(x['start'] + 1) + ' to ' + str(x['end']) + ' at ' + str(dt_friendly),
                  'MessageAttributes': {
                    'RangeStart': {
                      'DataType': 'Number',
                      'StringValue': str(x['start'] + 1)
                    },
                    'RangeEnd': {
                      'DataType': 'Number',
                      'StringValue': str(x['end'])
                    },
                    'TotalDevices': {
                      'DataType': 'Number',
                      'StringValue': str(x['end'] - x['start'])
                    },
                    'BatchId': {
                      'DataType': 'String',
                      'StringValue': x['batch_id']
                    },
                    'BatchTime': {
                      'DataType': 'Number',
                      'StringValue': str(x['batch_time'])
                    }
                  }
                }
                entries.append(entry)

              response = sqs_client.send_message_batch(
                QueueUrl=queue_url,
                Entries=entries
              )
            return(job_log)

          def lambda_handler(event, context):
            batch_id = str(uuid.uuid4())
            batch_time = int(datetime.now().timestamp() // 300 * 300) * 1000
            job_log = {}
            job_start = datetime.now()
            jobs = []

            for i in range(0, device_count, records_per_worker):
              job = {
                'batch_id': batch_id,
                'batch_time': batch_time,
                'start': i,
                'end': min((i + records_per_worker), device_count)}
              jobs.append(job)

            job_results = write_sqs(jobs)
            job_log.update(job_results)
            job_end = datetime.now()
            job_duration = job_end - job_start
            job_log['batch_id'] = str(batch_id)
            job_log['batch_time'] = batch_time
            job_log['start'] = job_start.replace(microsecond=0).isoformat()
            job_log['end'] = job_end.replace(microsecond=0).isoformat()
            job_log['duration_sec'] = job_duration.total_seconds()
            print(json.dumps(job_log))
Outputs:
  TimestreamDatabaseARN:
    Description: 'ARN of Timestream database'
    Value: !If [ createTimestream, !GetAtt TimestreamTable.Arn, !Join [ '/', [ !Join [ ':', ['arn' , 'aws', 'timestream', !Ref AWS::Region, !Ref AWS::AccountId, 'database'] ] , !Ref TimestreamDatabaseName ] ] ]
    Export:
      Name: !Join [ ':', [ !Ref AWS::StackName, 'TimestreamDatabaseARN' ] ]
  TimestreamTableARN:
    Description: 'ARN of Timestream table'
    Value: !If [ createTimestream, !GetAtt TimestreamTable.Arn, !Join [ '/', [ !Join [ ':', ['arn' , 'aws', 'timestream', !Ref AWS::Region, !Ref AWS::AccountId, 'database'] ] , !Ref TimestreamDatabaseName, 'table', !Ref TimestreamTableName ] ] ]
    Export:
      Name: !Join [ ':', [ !Ref AWS::StackName, 'TimestreamTableARN' ] ]
  TimestreamDatabaseName:
    Description: 'Name of Timestream database'
    Value: !If [createTimestream, !Ref TimestreamDatabase, !Ref TimestreamDatabaseName]
    Export:
      Name: !Join [ ':', [ !Ref AWS::StackName, 'TimestreamDatabaseName' ] ]
  TimestreamTableName:
    Description: 'Name of Timestream table'
    Value: !If [createTimestream, !GetAtt TimestreamTable.Name, !Ref TimestreamTableName]
    Export:
      Name: !Join [ ':', [ !Ref AWS::StackName, 'TimestreamTableName' ] ]